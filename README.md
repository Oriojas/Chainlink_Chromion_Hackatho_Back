# ğŸŒ¤ï¸ Sistema de PredicciÃ³n ClimÃ¡tica con LLM

Microservicio basado en FastAPI que proporciona pronÃ³sticos meteorolÃ³gicos interpretados por Inteligencia Artificial usando un LLM (Large Language Model) local.

## ğŸ¯ CaracterÃ­sticas Principales

- **FastAPI backend** para alto rendimiento
- **IntegraciÃ³n con OpenWeatherMap API** para datos meteorolÃ³gicos precisos
- **AnÃ¡lisis con LLM local** para interpretaciones y recomendaciones inteligentes
- **Formato JSON** para fÃ¡cil integraciÃ³n
- **ConfiguraciÃ³n flexible** mediante variables de entorno
- **Endpoints simples y documentados**

## ğŸš€ Endpoints Disponibles

### 1. `/prediction` - PronÃ³stico BÃ¡sico

Devuelve datos meteorolÃ³gicos en formato JSON.

**MÃ©todo:** GET  
**ParÃ¡metros:**
- `lat` (float): Latitud de la ubicaciÃ³n
- `lon` (float): Longitud de la ubicaciÃ³n

**Ejemplo:**
```bash
curl "http://localhost:8000/prediction?lat=4.60971&lon=-74.08175"
```

**Respuesta:**
```json
{
  "pronostico": [
    {
      "fecha": "12/06 22:00",
      "temperatura": "11.36Â°C",
      "descripcion": "Muy nuboso",
      "prob_precipitacion": "8.0%"
    }
  ]
}
```

### 2. `/prediction-llm` - PronÃ³stico con AnÃ¡lisis IA

Combina datos meteorolÃ³gicos con anÃ¡lisis interpretativo de un LLM local.

**MÃ©todo:** GET  
**ParÃ¡metros:**
- `lat` (float): Latitud de la ubicaciÃ³n
- `lon` (float): Longitud de la ubicaciÃ³n
- `llm_hash` (string, opcional): Hash ID del modelo LLM

**Ejemplo:**
```bash
# Con hash especÃ­fico
curl "http://localhost:8000/prediction-llm?lat=4.60971&lon=-74.08175&llm_hash=tu-hash-aqui"

# Con hash por defecto
curl "http://localhost:8000/prediction-llm?lat=4.60971&lon=-74.08175"
```

**Respuesta:**
```json
{
  "success": true,
  "prediccion_interpretada": {
    "response": "BasÃ¡ndome en los datos meteorolÃ³gicos...",
    "analysis": "Recomendaciones y anÃ¡lisis del LLM"
  },
  "datos_clima": {
    "pronostico": [...]
  },
  "mensaje": "PredicciÃ³n generada exitosamente con anÃ¡lisis de LLM"
}
```

## ğŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### Requisitos
- Python 3.8+
- API key de OpenWeatherMap (gratuita)
- LLM local ejecutÃ¡ndose (para funcionalidad IA)

### 1. Clonar el repositorio
```bash
git clone https://github.com/Oriojas/Chainlink_Chromion_Hackatho_Backn.git
cd Chainlink_Chromion_Hackatho_Backn
```

### 2. Instalar dependencias
```bash
pip install -r requirements.txt
# o usando poetry
poetry install
```

### 3. Configurar variables de entorno
Crea un archivo `.env` basado en `.env.example`:

```bash
# API Key de OpenWeatherMap
OPENWEATHER_APP_KEY=tu_api_key_aqui

# ConfiguraciÃ³n del servidor
FASTAPI_HOST=0.0.0.0
FASTAPI_PORT=8000

# ConfiguraciÃ³n del LLM local (opcional)
LLM_BASE_URL=http://localhost:3001
LLM_HASH_ID=tu-hash-del-modelo-aqui
LLM_TIMEOUT=30
```

### 4. Obtener API Key de OpenWeatherMap
1. Visita [OpenWeatherMap](https://openweathermap.org/api)
2. RegÃ­strate gratuitamente
3. ObtÃ©n tu API key
4. AgrÃ©gala al archivo `.env`

## ğŸš€ Uso

### Iniciar el servidor
```bash
python main.py
```

El servidor estarÃ¡ disponible en: `http://localhost:8000`

### DocumentaciÃ³n interactiva
Visita `http://localhost:8000/docs` para la documentaciÃ³n automÃ¡tica de Swagger.

## ğŸ¤– ConfiguraciÃ³n del LLM

### Requisitos para funcionalidad IA:
1. **LLM local ejecutÃ¡ndose** en `http://localhost:3001`
2. **Hash del modelo** (especÃ­fico para cada ejecuciÃ³n)
3. **Endpoint funcional:** `http://localhost:3001/{hash}/message`

### âš ï¸ Importante sobre el Hash:
- El hash **cambia en cada ejecuciÃ³n** del modelo LLM
- Debes **obtener el hash actual** desde la interfaz de tu LLM
- Ejemplo: `dd1a3913-6f2b-060b-9d69-7efb4bce9f01`

### ConfiguraciÃ³n del Hash:

#### OpciÃ³n 1: Variable de entorno (recomendado)
```bash
LLM_HASH_ID=tu-hash-del-modelo-aqui
```

#### OpciÃ³n 2: ParÃ¡metro en la URL
```bash
curl "http://localhost:8000/prediction-llm?lat=4.60971&lon=-74.08175&llm_hash=tu-hash-aqui"
```

## ğŸ§ª Pruebas

### Suite de pruebas automatizada
```bash
python test/test_weather_prediction.py
```

Esta suite de pruebas incluye:
- âœ… VerificaciÃ³n de conexiÃ³n con FastAPI
- âœ… Prueba del endpoint bÃ¡sico `/prediction`
- âœ… Prueba del endpoint con IA `/prediction-llm`
- âœ… VerificaciÃ³n de conexiÃ³n con LLM
- âœ… Pruebas con mÃºltiples ubicaciones

### Pruebas manuales
```bash
# Probar endpoint bÃ¡sico
curl "http://localhost:8000/prediction?lat=4.60971&lon=-74.08175"

# Probar endpoint con LLM
curl "http://localhost:8000/prediction-llm?lat=4.60971&lon=-74.08175&llm_hash=tu-hash"
```

## ğŸ“ Estructura del Proyecto

```
Chainlink_Chromion_Hackatho_Backn/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ queries.py          # LÃ³gica de consultas meteorolÃ³gicas y LLM
â”‚   â”œâ”€â”€ pronostico.json     # Ejemplo de salida
â”‚   â””â”€â”€ __pycache__/        # Cache de Python
â”œâ”€â”€ test/
â”‚   â””â”€â”€ test_weather_prediction.py  # Suite de pruebas unificada
â”œâ”€â”€ main.py                 # AplicaciÃ³n FastAPI principal
â”œâ”€â”€ .env.example           # Plantilla de configuraciÃ³n
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md              # Este archivo
â”œâ”€â”€ poetry.lock
â””â”€â”€ pyproject.toml         # ConfiguraciÃ³n de dependencias
```

## ğŸ”§ Funcionalidades TÃ©cnicas

### Procesamiento de Datos ClimÃ¡ticos
- Obtiene pronÃ³stico extendido (5 dÃ­as, intervalos de 3 horas)
- Formatea fechas y datos meteorolÃ³gicos
- Extrae informaciÃ³n relevante (temperatura, descripciÃ³n, precipitaciÃ³n)

### IntegraciÃ³n con LLM
- Crea texto descriptivo optimizado para el LLM
- EnvÃ­a solicitudes HTTP al LLM local
- Maneja errores de conexiÃ³n y timeouts
- Devuelve respuestas estructuradas

### AnÃ¡lisis Solicitado al LLM
El sistema solicita al LLM:
- ğŸ“Š AnÃ¡lisis del patrÃ³n climÃ¡tico general
- ğŸ‘• Recomendaciones de vestimenta y actividades
- âš ï¸ Alertas o precauciones importantes
- ğŸ“ˆ PredicciÃ³n de tendencias futuras

## ğŸŒ Ejemplos de Uso

### Ciudades de Ejemplo

#### BogotÃ¡, Colombia
```bash
curl "http://localhost:8000/prediction-llm?lat=4.60971&lon=-74.08175"
```

#### Madrid, EspaÃ±a
```bash
curl "http://localhost:8000/prediction-llm?lat=40.4168&lon=-3.7038"
```

#### Ciudad de MÃ©xico, MÃ©xico
```bash
curl "http://localhost:8000/prediction-llm?lat=19.4326&lon=-99.1332"
```

## âš ï¸ Manejo de Errores

### Errores Comunes y Soluciones

#### 1. Error de API del clima
```json
{
  "success": false,
  "error": "No se pudo obtener el pronÃ³stico del clima"
}
```
**SoluciÃ³n:** Verifica tu API key de OpenWeatherMap.

#### 2. Error de conexiÃ³n con LLM
```json
{
  "success": false,
  "error": "Error al consultar LLM local: [detalle]"
}
```
**SoluciÃ³n:** Verifica que el LLM estÃ© ejecutÃ¡ndose y el hash sea correcto.

#### 3. Timeout del LLM
**SoluciÃ³n:** Aumenta el valor de `LLM_TIMEOUT` en las variables de entorno.

## ğŸš¨ SoluciÃ³n de Problemas

### LLM no responde:
1. âœ… Verifica que el LLM estÃ© en `http://localhost:3001`
2. ğŸ”‘ **Confirma que el hash del modelo sea correcto** (error mÃ¡s comÃºn)
3. ğŸ”— Verifica el endpoint: `http://localhost:3001/{hash}/message`
4. ğŸ“‹ Revisa los logs para errores de conexiÃ³n
5. ğŸ§ª Prueba el LLM directamente con curl

### API del clima no funciona:
1. ğŸ”‘ Verifica tu API key de OpenWeatherMap
2. ğŸ“ Confirma que las coordenadas sean vÃ¡lidas (-90 a 90 lat, -180 a 180 lon)
3. ğŸ“Š Revisa tu cuota de API calls

### Servidor no inicia:
1. ğŸ Verifica que Python 3.8+ estÃ© instalado
2. ğŸ“¦ Instala las dependencias: `pip install -r requirements.txt`
3. ğŸ”§ Verifica el archivo `.env`

## ğŸ”„ Flujo de Trabajo del Sistema

```
Cliente solicita /prediction-llm
        â†“
Obtener datos climÃ¡ticos (OpenWeatherMap)
        â†“
Procesar datos meteorolÃ³gicos
        â†“
Crear texto optimizado para LLM
        â†“
Consultar LLM local
        â†“
Combinar respuestas
        â†“
Retornar resultado completo
```

## ğŸ”œ Mejoras Futuras

- ğŸ“š CachÃ© de respuestas del LLM
- ğŸ”„ MÃºltiples LLMs para comparaciÃ³n
- ğŸ˜Š AnÃ¡lisis de sentimientos del clima
- ğŸŒ IntegraciÃ³n con mÃ¡s APIs meteorolÃ³gicas
- ğŸ‘¤ PersonalizaciÃ³n de prompts por usuario
- ğŸ“± API para aplicaciones mÃ³viles

## ğŸ¤ ContribuciÃ³n

Este proyecto forma parte del **Chainlink Chromion Hackathon**.

### Repositorio del Agente IA
https://github.com/Oriojas/eliza-starter-orc.git

### CÃ³mo contribuir:
1. Fork el repositorio
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request

## ğŸ“„ Licencia

MIT License - Ver archivo [LICENSE](LICENSE) para mÃ¡s detalles.

## ğŸ“ Soporte

Si tienes problemas:
1. ğŸ“– Revisa esta documentaciÃ³n
2. ğŸ§ª Ejecuta las pruebas: `python test/test_weather_prediction.py`
3. ğŸ“‹ Revisa los logs del servidor
4. ğŸ› Reporta issues en GitHub

---

**Â¡Gracias por usar el Sistema de PredicciÃ³n ClimÃ¡tica con LLM!** ğŸŒ¤ï¸ğŸ¤–